{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Review #3 - Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys, os\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dot, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from utils import MultipleTimeSeriesCV, format_time\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "results_path = Path('KR2_results', 'asset_pricing')\n",
    "if not results_path.exists():\n",
    "    results_path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characteristics = ['beta', 'betasq', 'chmom', 'krwvol', 'idiovol', 'ill', 'indmom',\n",
    "                   'maxret', 'mom12m', 'mom1m', 'mom36m', 'mvel', 'retvol', 'turn', 'turn_std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(results_path / 'autoencoder.h5') as store:\n",
    "    print(store.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (pd.read_hdf(results_path / 'autoencoder.h5', 'returns')\n",
    "        .stack(dropna=False)\n",
    "        .to_frame('returns')\n",
    "        .loc[idx['2006':, :], :])\n",
    "\n",
    "with pd.HDFStore(results_path / 'autoencoder.h5') as store:\n",
    "    keys = [k[1:] for k in store.keys() if k[1:].startswith('factor')]\n",
    "    for key in keys:\n",
    "        data[key.split('/')[-1]] = store[key].squeeze()\n",
    "        \n",
    "characteristics = data.drop('returns', axis=1).columns.tolist()\n",
    "data['returns_fwd'] = data.returns.unstack('ticker').shift(-1).stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank-normalize characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, characteristics] = (data.loc[:, characteristics]\n",
    "                                .groupby(level='date')\n",
    "                                .apply(lambda x: pd.DataFrame(quantile_transform(x, \n",
    "                                                                                 copy=True, \n",
    "                                                                                 n_quantiles=x.shape[0]),\n",
    "                                                              columns=characteristics,\n",
    "                                                              index=x.index.get_level_values('ticker')))\n",
    "                               .mul(2).sub(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors = 3\n",
    "n_characteristics = len(characteristics)\n",
    "n_tickers = len(data.index.unique('ticker'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_beta = Input((n_tickers, n_characteristics), name='input_beta')\n",
    "input_factor = Input((n_tickers,), name='input_factor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Characteristics Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = Dense(units=8, activation='relu', name='hidden_layer')(input_beta)\n",
    "batch_norm = BatchNormalization(name='batch_norm')(hidden_layer)\n",
    "output_beta = Dense(units=n_factors, name='output_beta')(batch_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfo_factor = Dense(units=n_characteristics, name='pfo_factor')(input_factor)\n",
    "output_factor = Dense(units=n_factors, name='output_factor')(pfo_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Dot(axes=(2,1), name='output_layer')([output_beta, output_factor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_beta, input_factor], outputs=output)\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automate model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(hidden_units=8, n_factors=3):\n",
    "    input_beta = Input((n_tickers, n_characteristics), name='input_beta')\n",
    "    input_factor = Input((n_tickers,), name='input_factor')\n",
    "\n",
    "    hidden_layer = Dense(units=hidden_units, activation='relu', name='hidden_layer')(input_beta)\n",
    "    batch_norm = BatchNormalization(name='batch_norm')(hidden_layer)\n",
    "    \n",
    "    output_beta = Dense(units=n_factors, name='output_beta')(batch_norm)\n",
    "    \n",
    "    pfo_factor = Dense(units=n_characteristics, name='pfo_factor')(input_factor)\n",
    "    output_factor = Dense(units=n_factors, name='output_factor')(pfo_factor)\n",
    "\n",
    "    output = Dot(axes=(2,1), name='output_layer')([output_beta, output_factor])\n",
    "\n",
    "    model = Model(inputs=[input_beta, input_factor], outputs=output)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 52\n",
    "cv = MultipleTimeSeriesCV(n_splits=5, # 5\n",
    "                          train_period_length=15*YEAR,\n",
    "                          test_period_length=1*YEAR,\n",
    "                          lookahead=1)\n",
    "\n",
    "def get_train_valid_data(data, train_idx, val_idx):\n",
    "    train, val = data.iloc[train_idx], data.iloc[val_idx]\n",
    "    X1_train = train.loc[:, characteristics].values.reshape(-1, n_tickers, n_characteristics)\n",
    "    X1_val = val.loc[:, characteristics].values.reshape(-1, n_tickers, n_characteristics)\n",
    "    X2_train = train.loc[:, 'returns'].unstack('ticker')\n",
    "    X2_val = val.loc[:, 'returns'].unstack('ticker')\n",
    "    y_train = train.returns_fwd.unstack('ticker')\n",
    "    y_val = val.returns_fwd.unstack('ticker')\n",
    "    return X1_train, X2_train, y_train, X1_val, X2_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_opts = [2, 3, 4, 5, 6] # 2, 3, 4, 5, 6\n",
    "unit_opts = [8, 16, 32] # 8, 16, 32\n",
    "param_grid = list(product(unit_opts, factor_opts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "cols = ['units', 'n_factors', 'fold', 'epoch', 'ic_mean', \n",
    "        'ic_daily_mean', 'ic_daily_std', 'ic_daily_median']\n",
    "\n",
    "import gc\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from keras.utils import Sequence\n",
    "\n",
    "# Define a custom data generator to feed data to the model\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X1, X2, y, batch_size):\n",
    "        self.X1 = X1\n",
    "        self.X2 = X2\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X1) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_X1 = self.X1[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_X2 = self.X2[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return [batch_X1, batch_X2], batch_y\n",
    "\n",
    "class ClearMemory(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()\n",
    "        k.clear_session()\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "# define early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "for units, n_factors in param_grid:\n",
    "    print(\"units:\", units, \", n_factors:\", n_factors)\n",
    "    scores = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(data)):\n",
    "        X1_train, X2_train, y_train, X1_val, X2_val, y_val = get_train_valid_data(data,\n",
    "                                                                                  train_idx,\n",
    "                                                                                  val_idx)\n",
    "        train_gen = DataGenerator(X1_train, X2_train, y_train, batch_size=batch_size)\n",
    "        val_gen = DataGenerator(X1_val, X2_val, y_val, batch_size=batch_size)\n",
    "        model = make_model(hidden_units=units, n_factors=n_factors)\n",
    "        for epoch in range(250):\n",
    "            model.fit_generator(train_gen,\n",
    "                                validation_data=val_gen,\n",
    "                                epochs=epoch + 1,\n",
    "                                initial_epoch=epoch, verbose=False, shuffle=True, callbacks=ClearMemory())\n",
    "            y_pred = model.predict_generator(val_gen, callbacks=ClearMemory())\n",
    "            y_true = y_val.stack().values\n",
    "            date_index = y_val.stack().index\n",
    "            result = (pd.DataFrame({'y_pred': y_pred.reshape(-1),\n",
    "                                    'y_true': y_true},\n",
    "                                   index=date_index)\n",
    "                      .replace(-2, np.nan).dropna())\n",
    "            r0 = spearmanr(result.y_true, result.y_pred)[0]\n",
    "            r1 = result.groupby(level='date').apply(lambda x: spearmanr(x.y_pred,\n",
    "                                                                        x.y_true)[0])\n",
    "\n",
    "            scores.append([units, n_factors, fold, epoch, r0,\n",
    "                           r1.mean(), r1.std(), r1.median()])\n",
    "            if epoch % 50 == 0:\n",
    "                print(f'{format_time(time()-start)} | {n_factors} | {units:02} | {fold:02}-{epoch:03} | {r0:6.2%} | '\n",
    "                  f'{r1.mean():6.2%} | {r1.median():6.2%}')\n",
    "            \n",
    "        scores = pd.DataFrame(scores, columns=cols)\n",
    "        scores.to_hdf(results_path / 'scores.h5', f'{units}/{n_factors}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "with pd.HDFStore(results_path / 'scores.h5') as store:\n",
    "    for key in store.keys():\n",
    "        scores.append(store[key])\n",
    "print(scores)\n",
    "scores = pd.concat(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = (scores.groupby(['n_factors', 'units', 'epoch'])\n",
    "       ['ic_mean', 'ic_daily_mean', 'ic_daily_median']\n",
    "       .mean()\n",
    "      .reset_index())\n",
    "\n",
    "top = (avg.groupby(['n_factors', 'units'])\n",
    "       .apply(lambda x: x.nlargest(n=5, columns=['ic_daily_median']))\n",
    "       .reset_index(-1, drop=True))\n",
    "\n",
    "avg.nlargest(n=50, columns=['ic_daily_median'])\n",
    "top.nlargest(n=5, columns=['ic_daily_median'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors = 3\n",
    "units = 8\n",
    "batch_size = 64\n",
    "first_epoch = 50\n",
    "last_epoch = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for epoch in tqdm(list(range(first_epoch, last_epoch))):\n",
    "    epoch_preds = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(data)):\n",
    "        X1_train, X2_train, y_train, X1_val, X2_val, y_val = get_train_valid_data(data,\n",
    "                                                                                  train_idx,\n",
    "                                                                                  val_idx)\n",
    "        \n",
    "        train_gen = DataGenerator(X1_train, X2_train, y_train, batch_size=batch_size)\n",
    "        val_gen = DataGenerator(X1_val, X2_val, y_val, batch_size=batch_size)\n",
    "        \n",
    "        model.fit_generator(train_gen,\n",
    "                    validation_data=val_gen,\n",
    "                  epochs=epoch,\n",
    "                  verbose=0,\n",
    "                  shuffle=True, callbacks=[ClearMemory(), early_stop])\n",
    "        epoch_preds.append(pd.Series(model.predict_generator(val_gen, callbacks=[ClearMemory(), early_stop]).reshape(-1),\n",
    "                                     index=y_val.stack().index).to_frame(epoch))\n",
    "\n",
    "    predictions.append(pd.concat(epoch_preds))\n",
    "    \n",
    "    predictions_combined = pd.concat(predictions, axis=1).sort_index()\n",
    "    predictions_combined.to_hdf(results_path / 'predictions.h5', 'predictions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
